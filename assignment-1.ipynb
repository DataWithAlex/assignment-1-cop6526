{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework-1 (COP-6526) | Alex Sciuto\n",
    "\n",
    "## Problem-1. Compare the following two Python codes, which sum all elments in a big vector. \n",
    "\n",
    "I added some additional code lines to be able to calculate the Real Time (Wall-clock time), User Time, and System Time as discussed in our last lecture. _**NOTE: It is the exact same code with extra logs, the operations were not modified**_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computation time for Version 1 Code:\n",
      "Total Sum: 4999999950000000\n",
      "Real Time (Wall-clock time): 12.366380 seconds\n",
      "User Time: 12.083830 seconds\n",
      "System Time: 0.085803 seconds\n"
     ]
    }
   ],
   "source": [
    "import array, time, resource\n",
    "\n",
    "# Create a large array of 100 million elements\n",
    "arr = array.array('l', range(100000000))\n",
    "total_sum = 0\n",
    "\n",
    "# Start timing\n",
    "begin_real = time.time()  # Wall-clock time (real time)\n",
    "begin_user, begin_sys = resource.getrusage(resource.RUSAGE_SELF)[:2]  # User and system time\n",
    "\n",
    "# Perform the summation\n",
    "for i in range(10000):\n",
    "    for j in range(10000):\n",
    "        total_sum += arr[i * 10000 + j]\n",
    "\n",
    "# Stop timing\n",
    "end_real = time.time()\n",
    "end_user, end_sys = resource.getrusage(resource.RUSAGE_SELF)[:2]\n",
    "\n",
    "# Print the computed sum\n",
    "print(\"Computation time for Version 1 Code:\")\n",
    "print(f\"Total Sum: {total_sum}\")\n",
    "\n",
    "# Print the time measurements\n",
    "print(f\"Real Time (Wall-clock time): {end_real - begin_real:.6f} seconds\")\n",
    "print(f\"User Time: {end_user - begin_user:.6f} seconds\")\n",
    "print(f\"System Time: {end_sys - begin_sys:.6f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computation time for Version 2 Code:\n",
      "Total Sum: 4999999950000000\n",
      "Real Time (Wall-clock time): 13.777952 seconds\n",
      "User Time: 13.306791 seconds\n",
      "System Time: 0.173770 seconds\n"
     ]
    }
   ],
   "source": [
    "import array, time, resource\n",
    "\n",
    "# Create a large array of 100 million elements\n",
    "arr = array.array('l', range(100000000))\n",
    "total_sum = 0\n",
    "\n",
    "# Start timing\n",
    "begin_real = time.time()  # Start wall-clock time (real time)\n",
    "begin_user, begin_sys = resource.getrusage(resource.RUSAGE_SELF)[:2]  # Start user and system time\n",
    "\n",
    "# Perform the summation\n",
    "for i in range(10000):\n",
    "    for j in range(10000):\n",
    "        total_sum += arr[j * 10000 + i]\n",
    "\n",
    "# Stop timing\n",
    "end_real = time.time()  # End wall-clock time\n",
    "end_user, end_sys = resource.getrusage(resource.RUSAGE_SELF)[:2]  # End user and system time\n",
    "\n",
    "# Print the computed sum\n",
    "print(\"Computation time for Version 2 Code:\")\n",
    "print(f\"Total Sum: {total_sum}\")\n",
    "\n",
    "# Print the time measurements\n",
    "print(f\"Real Time (Wall-clock time): {end_real - begin_real:.6f} seconds\")\n",
    "print(f\"User Time: {end_user - begin_user:.6f} seconds\")\n",
    "print(f\"System Time: {end_sys - begin_sys:.6f} seconds\")\n",
    "\n",
    "\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question-1 (10 points): Which code use cache better? Why? Please test the execution time of the two code versions, which code runs faster? Does this result match your original thought?\n",
    "\n",
    "After running the two versions of the code, we find that `version 1` utilizes cache more efficiently from a lower completion time from `version 1` (11.70s) compared to `version 2` (13.49). The reason this is the case has to do with how the loop is formed. \n",
    "\n",
    "`Version 1` is accessing the array in a sequential order `arr[i * 1000 + j]` which means that the data is being stored contiguously in memory allowing for CPU cache to load a block of data once and reuse it multiple times. This is because this code leverages the phenomenon known as **Spatial Locality**, where consectuvie memory access are likely to be within the same cacheline, minimizing cache misses. \n",
    "\n",
    "Conversly, `Version 2` is accessing the array in a strided pattern `arr[j * 1000 + i]` which means that the code needs to jump around in memory while doing the computation. This suggests that the code is not leveraging Spatial Locaility well, leading to more frequent cache misses and therefore lower performance. \n",
    "\n",
    "In terms of my original thought, I did not have a preconception for which one would run faster. I did not consider how switching the loop from `arr[i * 1000 + j]` to `arr[j * 1000 + i]` would have an impact on how cache is utilized, for I thought the python intepreter has its own mechanisims for increasing cache efficiency. \n",
    "\n",
    "### Question-2 (10 points): Does Python utilize CPU cache well? You can investigate this topic by reading online discussion using google.\n",
    "\n",
    "What I read is that Python is implemented through `CPython`, which actively translates python code into C and compiles it into bytecode. This is different from lower level languages like `C` which directly do the operations, which make it more efficient. Here are some specific details that make Python inefficient at utilizing CPU Cache. \n",
    "\n",
    "Object Allocation and Deallocation: Allocation refers to the process of reserving a block of memory for a program to use, where if a new variable is made, it is where it is stored. Whereas Deallocation refers to freeing up that memory for it to be used by another program. Python frequently allocates and deallocates objects in an inefficient manner, where every time a new object is made or removed, it requires several memory operations and disrupts cache locality. \n",
    "\n",
    "Variable-Sized Integers: lower level languages like `C` have fixed-size integers which help with cache efficiency. Python defaults to variable-sized integers which uses more memory, and decrease cache efficiency. Whenever python does operations on these variable-sized integers, additional memory access is required beyond the cache to manage the objects. \n",
    "\n",
    "Global Interpreter Lock (GIL): Python uses GIL so only one thread can be used at a time to execute python bytecode. We have computers that have multiple cores (e.g., my Mac has access to 10 cores), and Python is unable to maximize CPU efficiency across the cores when performing operations. Cache operations specifically benefit from multi-threading, which can't occur when running python code due to the GIL. \n",
    "\n",
    "### Question-3 (10 points): A much more efficient way is to use \"NumPy\". Please use NumPy to do the same computation. What's the execution time? Why does it run much faster?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Version 1 Code with NumPy:\n",
      "Total Sum: 4999999950000000\n",
      "Time taken: 0.060395 seconds\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Create a large NumPy array of 100 million elements\n",
    "arr = np.arange(100000000, dtype=np.int64)\n",
    "\n",
    "# Start timing\n",
    "begin = time.time()\n",
    "\n",
    "# Perform the summation using a vectorized operation\n",
    "total_sum = np.sum(arr.reshape(10000, 10000))\n",
    "\n",
    "# Print the computed sum\n",
    "print(\"For Version 1 Code with NumPy:\")\n",
    "print(f\"Total Sum: {total_sum}\")\n",
    "\n",
    "# Print the total time taken\n",
    "print(f\"Time taken: {time.time() - begin:.6f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Version 2 Code with NumPy:\n",
      "Total Sum: 4999999950000000\n",
      "Time taken: 0.069948 seconds\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Create a large NumPy array of 100 million elements\n",
    "arr = np.arange(100000000, dtype=np.int64)\n",
    "\n",
    "# Start timing\n",
    "begin = time.time()\n",
    "\n",
    "# Perform the summation using a different access pattern\n",
    "total_sum = np.sum(arr.reshape(10000, 10000).T)\n",
    "\n",
    "# Print the computed sum\n",
    "print(\"For Version 2 Code with NumPy:\")\n",
    "print(f\"Total Sum: {total_sum}\")\n",
    "\n",
    "# Print the total time taken\n",
    "print(f\"Time taken: {time.time() - begin:.6f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question-3 (10 points): A much more efficient way is to use \"NumPy\". Please use NumPy to do the same computation. What's the execution time? Why does it run much faster?\n",
    "\n",
    "When we are comparing the execution times, we find that for both code blocks it is significantly faster using numpy. For the `version 1` code, without numpy it compiled in 11.70s and with it 0.060 (i.e., 195x faster); For the `version 2` code, without numpy is compiled in 13.49s and with it 0.069s (i.e., 195x faster). \n",
    "\n",
    "In terms of why it is much faster, here are a few explanations: \n",
    "\n",
    "numpy's use of vectorization: numPy leverages vectorized operations where it applied a function to an entire array at once, reather than iterating through each element one-by-one. This reduces the overhead associated with python loops, allowing for much faster computation time. \n",
    "\n",
    "Memory Contiguity: Since numPy uses arrays, they are stored in contiguous blocks of memory which is cache-friendly. This reduces the number of cache misses compared to using the default python arrays, \n",
    "\n",
    "Underlying C Implementation: NumPy is direectly implemented in C, which allows use to leverage specific CPU-level instructions. For instance, NumPy specifies that variables should be fixed-size data types (e.g., int64) which minimizes memory allocation and improves cache utilization. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem-2. (40 points) Implement a parallel matrix-vector multiplication in Python using MPI. You may generate a randon matrix and a corresponding vector, then broadcast the vector and scatter the matrix to all processes, and finally gather the result back. \n",
    "\n",
    "In order to complete this operation I am going to use the `mpi4py` python library. When using this package, you need to directly run the code in terminal using the `mpiexec` command. I created a python file called `matrix-multiplication.py` and I am going to run it using two cores. I have also added detailed logging to understand the sequential process of how the code completed the vector multiplication. \n",
    "\n",
    "Below are the contents of `matrix-multiplication.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "import logging\n",
    "import time  # Import the time module for tracking computation time\n",
    "\n",
    "# Initialize MPI\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.DEBUG, format=\"%(asctime)s [%(levelname)s] %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\")\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Matrix and vector sizes\n",
    "n = 1000  # Change this size for larger matrices\n",
    "\n",
    "# Start tracking total computation time\n",
    "total_start_time = time.time()\n",
    "\n",
    "if rank == 0:\n",
    "    # Master process: generate random matrix and vector\n",
    "    matrix = np.random.rand(n, n)\n",
    "    vector = np.random.rand(n)\n",
    "    logger.info(f\"Rank {rank}: Generated random matrix and vector.\")\n",
    "else:\n",
    "    # Other processes: initialize empty matrix and vector\n",
    "    matrix = None\n",
    "    vector = np.empty(n, dtype='d')\n",
    "    logger.info(f\"Rank {rank}: Initialized empty matrix and vector placeholders.\")\n",
    "\n",
    "# Broadcast the vector to all processes\n",
    "logger.info(f\"Rank {rank}: Broadcasting vector from rank 0 to all processes.\")\n",
    "comm.Bcast(vector, root=0)\n",
    "logger.info(f\"Rank {rank}: Broadcast completed.\")\n",
    "\n",
    "# Scatter the matrix to all processes\n",
    "rows_per_process = n // size\n",
    "local_matrix = np.empty((rows_per_process, n), dtype='d')\n",
    "logger.info(f\"Rank {rank}: Scattering matrix: each process will receive {rows_per_process} rows.\")\n",
    "comm.Scatter(matrix, local_matrix, root=0)\n",
    "logger.info(f\"Rank {rank}: Matrix scatter completed.\")\n",
    "\n",
    "# Start timing the local computation\n",
    "local_start_time = time.time()\n",
    "\n",
    "# Perform local matrix-vector multiplication\n",
    "logger.info(f\"Rank {rank}: Performing local matrix-vector multiplication.\")\n",
    "local_result = np.dot(local_matrix, vector)\n",
    "\n",
    "# End timing the local computation\n",
    "local_end_time = time.time()\n",
    "local_computation_time = local_end_time - local_start_time\n",
    "\n",
    "logger.info(f\"Rank {rank}: Local result computed: {local_result[:5]}...\")  # Print only the first 5 elements for brevity\n",
    "logger.info(f\"Rank {rank}: Local computation time: {local_computation_time:.6f} seconds\")\n",
    "\n",
    "# Gather the local results back to the master process\n",
    "if rank == 0:\n",
    "    result = np.empty(n, dtype='d')\n",
    "else:\n",
    "    result = None\n",
    "\n",
    "logger.info(f\"Rank {rank}: Gathering results from all processes.\")\n",
    "comm.Gather(local_result, result, root=0)\n",
    "\n",
    "# End tracking total computation time\n",
    "total_end_time = time.time()\n",
    "total_computation_time = total_end_time - total_start_time\n",
    "\n",
    "if rank == 0:\n",
    "    logger.info(f\"Rank {rank}: Result of matrix-vector multiplication gathered: {result[:5]}...\")  # Print only the first 5 elements for brevity\n",
    "    logger.info(f\"Rank {rank}: Total computation time: {total_computation_time:.6f} seconds\")\n",
    "    print(\"Result of matrix-vector multiplication is computed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have `matrix-multiplication.py`, we are going to run it in the terminal using the command below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-17 12:36:28 [INFO] Rank 1: Initialized empty matrix and vector placeholders.\n",
      "2024-09-17 12:36:28 [INFO] Rank 1: Broadcasting vector from rank 0 to all processes.\n",
      "2024-09-17 12:36:28 [INFO] Rank 0: Generated random matrix and vector.\n",
      "2024-09-17 12:36:28 [INFO] Rank 0: Broadcasting vector from rank 0 to all processes.\n",
      "2024-09-17 12:36:28 [INFO] Rank 0: Broadcast completed.\n",
      "2024-09-17 12:36:28 [INFO] Rank 0: Scattering matrix: each process will receive 500 rows.\n",
      "2024-09-17 12:36:28 [INFO] Rank 1: Broadcast completed.\n",
      "2024-09-17 12:36:28 [INFO] Rank 1: Scattering matrix: each process will receive 500 rows.\n",
      "2024-09-17 12:36:28 [INFO] Rank 1: Matrix scatter completed.\n",
      "2024-09-17 12:36:28 [INFO] Rank 1: Performing local matrix-vector multiplication.\n",
      "2024-09-17 12:36:28 [INFO] Rank 0: Matrix scatter completed.\n",
      "2024-09-17 12:36:28 [INFO] Rank 0: Performing local matrix-vector multiplication.\n",
      "2024-09-17 12:36:28 [INFO] Rank 1: Local result computed: [248.56277831 243.65929634 256.8254286  253.48377645 245.10409473]...\n",
      "2024-09-17 12:36:28 [INFO] Rank 1: Local computation time: 0.000167 seconds\n",
      "2024-09-17 12:36:28 [INFO] Rank 1: Gathering results from all processes.\n",
      "2024-09-17 12:36:28 [INFO] Rank 0: Local result computed: [248.53301436 247.77791774 241.99700993 254.81127472 240.81912635]...\n",
      "2024-09-17 12:36:28 [INFO] Rank 0: Local computation time: 0.000339 seconds\n",
      "2024-09-17 12:36:28 [INFO] Rank 0: Gathering results from all processes.\n",
      "Result of matrix-vector multiplication is computed.\n",
      "2024-09-17 12:36:28 [INFO] Rank 0: Result of matrix-vector multiplication gathered: [248.53301436 247.77791774 241.99700993 254.81127472 240.81912635]...\n",
      "2024-09-17 12:36:28 [INFO] Rank 0: Total computation time: 0.016090 seconds\n"
     ]
    }
   ],
   "source": [
    "!mpiexec -n 2 python maxtrix-multiplication.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output details a parallel matrix-vector multiplication using MPI, where Rank 0 (the master process) generates a random matrix and vector, broadcasts the vector, and scatters parts of the matrix to all processes, including itself. Both Rank 0 and Rank 1 then perform their local matrix-vector multiplications and log the partial results, which are later gathered back to Rank 0.\n",
    "\n",
    "Timing information shows that Rank 1 completes its local computation in 0.000167 seconds, while Rank 0 takes 0.000339 seconds. The total time for the entire parallel operation, including all communication steps, is 0.016090 seconds, demonstrating efficient parallel processing and minimal overall execution time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
